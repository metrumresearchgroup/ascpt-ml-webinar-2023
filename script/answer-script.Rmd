---
title: "Modeling with xgboost"
subtitle: "ASCPT Webinar"
author: "Matthew Wiens, Metrum Research Group"
output: pdf_document
date: '2023-11-02'
editor_options: 
  chunk_output_type: console
---

# Setup

Load libraries and set up file paths.

- The tidyverse framework is used for data manipulation
- The tidymodels framework (https://www.tidymodels.org/) is used to manage models and the ML workflow
- xgboost is the package we'll be using to fit the models

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#load libraries 
library(tidyverse)
library(tidymodels)
library(xgboost)

data_dir <- here::here("data", "source")
```

Data source: https://archive.ics.uci.edu/ml/datasets/breast+cancer. Class is the dependent variable, and the goal is to classify patients based on the other features (covariates). There is one row per patient.

```{R}

# Pull names from the data description file, since the csv does not have column names
input_column_names <- c(
  "Class",
  "age",
  "menopause",
  "tumor_size",
  "inv_nodes",
  "node_caps",
  "deg_malig",
  "breast",
  "breast_quad",
  "irradiat"
)

# Load data, with options for this specific dataset
dat_raw <- read_csv(
  file = file.path(data_dir, "breast-cancer.data"),
  col_names = input_column_names,
  na = c("", "NA", "?")
) 
  

```

Data description:

   1. Class: no-recurrence-events, recurrence-events
   2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.
   3. menopause: lt40, ge40, premeno.
   4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44,
                  45-49, 50-54, 55-59.
   5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26,
                 27-29, 30-32, 33-35, 36-39.
   6. node-caps: yes, no.
   7. deg-malig: 1, 2, 3.
   8. breast: left, right.
   9. breast-quad: left-up, left-low, right-up, right-low, central.
  10. irradiat: yes, no.
  
Here's come additional (brief) descriptions of the data (Source: https://www.linkedin.com/pulse/using-machine-learning-techniques-predict-recurrence-breast-alva/). 

Age: age of the patient at the time of diagnosis;
Menopause: whether the patient is pre- or postmenopausal at time of diagnosis;
Tumor size: the greatest diameter (in mm) of the excised tumor;
Inv-nodes: the number (range 0 - 39) of axillary lymph nodes that contain metastatic breast cancer visible on histological examination;
Node caps: if the cancer does metastasise to a lymph node, although outside the original site of the tumor it may remain “contained” by the capsule of the lymph node. However, over time, and with more aggressive disease, the tumor may replace the lymph node and then penetrate the capsule, allowing it to invade the surrounding tissues;
Degree of malignancy: the histological grade (range 1-3) of the tumor. Tumors that are grade 1 predominantly consist of cells that, while neoplastic, retain many of their usual characteristics. Grade 3 tumors predominately consist of cells that are highly abnormal;
Breast: breast cancer may obviously occur in either breast;
Breast quadrant: the breast may be divided into four quadrants, using the nipple as a central point;
Irradiation: radiation therapy is a treatment that uses high-energy x-rays to destroy cancer cells. 

  
Cleanup the ordered categories to be numeric here  
```{R}

# Further preprocessing, using tidyverse
# Trees naturally handle ordered factors (categorical) covariates,
# so we code the data in that way
# The importance_weights column will be used later in the script, and is the (common class)/(rare class)
dat <- dat_raw %>%
  mutate(
    # define logical variable for whether subject had 
    # recurrence events (TRUE) or not (FALSE)
    class_n = Class == "recurrence-events",
    # define Class as a factor with levels that match class_n, 
    # so recurrence_events is factor level 1
    Class = factor(Class, 
                   levels = c("recurrence-events", "no-recurrence-events")),
    # define age as an ordered factor
    age_f = factor(age, ordered = TRUE),
    # define tumor size as an ordered factor with specified levels
    tumor_size_f = factor(tumor_size,
                          ordered = TRUE, 
                          levels = c("0-4", "5-9", "10-14", "15-19",
                                     "20-24", "25-29", "30-34", "35-39", 
                                     "40-44", "45-49",  "50-54")),
    # define nodes as an ordered factor with specified levels
    inv_nodes_f = factor(inv_nodes, 
                         ordered = TRUE,
                         levels = c("0-2", "3-5", "6-8", "9-11", 
                                    "12-14", "15-17", "24-26")),
    # define logical variable for whether subject receieved radiation (TRUE)
    # or not (FALSE)
    irradiat_n = irradiat == "yes",
    # define logical variable for whether node caps were used (TRUE) 
    # or not (FALSE)
    node_caps_n = node_caps == "yes",
    # define logical variable for whether tumor was in left breast (TRUE)
    # or not (FALSE)
    breast_left = breast == "left",
    # define importance weights of the rare as the ratio of the common to rare classes
    # from the parsnip package
    # This is a new feature, so if you have an older version of R (<4.0), comment this out
    case_weight = parsnip::importance_weights(if_else(class_n == 0, 1, 201/85))
  )

# Create the training/testing split with the tidymodels package
# will use default proportion for 75% (3/4) training, 25% testing
init_split <- initial_split(dat, prop = 3/4)

# Come back here and try 80/20, or 66/33 splits
# after fitting the model
# How do the fits and evaluation change?
```

# Fitting a model

Create the formatted datasets for training and testing in a format for **xgboost**, which is a matrix. 
Also, the **recipes** package is used to preprocess categorical and ordinal covariates into numerics for **xgboost**.

```{R}

# This is like specifying a model, but for data pre-proecessing 
# The data specification here just gives a template of the data for the transformations
preprocessing_formula <- as.formula(
  class_n ~ age_f + menopause + tumor_size_f + inv_nodes_f + node_caps + 
            deg_malig + breast + breast_quad + irradiat
)
preprocessing_xgboost <- recipes::recipe(
  preprocessing_formula, 
  # call the training portion of the split data set using training()
  # extraction function
  data = training(init_split)) %>%
  # convert the ordinal factors into numeric scores (e.g., 0, 1, 2)
  step_ordinalscore(age_f, tumor_size_f, inv_nodes_f) %>%
  # create dummy variables non-reference levels of factors, the one_hot = TRUE
  # option tells the function to assume a reference level and then make
  # numerics for the other levels. For example, if you have N levels, then
  # N-1 dummy variables are created.
  step_dummy(breast, breast_quad, irradiat, menopause, node_caps, one_hot = T)


# This is like fitting the model 
# The training set is specified because we want to learn the transformations
# on the training set only to avoid leaking any data from the test set
estimated_preprocessing <- prep(preprocessing_xgboost, training(init_split))

# This is like using the model to make predictions, 
# by applying the learned transformations to the datasets (training and testing)
# There will be warnings about new factor levels, in this case for xgboost we can ignore them
# Also, y is a logical vector, so the "pull" function is used
x_train <- bake(estimated_preprocessing, training(init_split), -class_n, composition = "matrix")
y_train <- bake(estimated_preprocessing, training(init_split)) %>% pull(class_n) 

x_test <- bake(estimated_preprocessing, testing(init_split), -class_n, composition = "matrix")
y_test <- bake(estimated_preprocessing, testing(init_split)) %>% pull(class_n) 

```

Fit the model with default tuning parameters. 

```{R}
# Fit the model, specifying that we are doing binary classification and thus
# want to "score" models to minimized their logistic loss. Note that
# this is not specifying logistic regression, rather specifying that the loss
# function to be minimized is logistic.
fit1 <- xgboost::xgboost(data = x_train,  # predictors # Make this a fill-in
                         label = y_train, # responses  # Make this a fill-in
                         # nrounds: number of boosted trees to add to ensemble
                         #          can be tuned, but fixed here 
                         # What happens if this increased? Try 30 or 100
                         nrounds = 15,  
                         # see note above about logistic loss function
                         params = list(objective = "binary:logistic"))

```

Next, we evaluate the fit. 

Wikipedia has a nice article (https://en.wikipedia.org/wiki/Sensitivity_and_specificity) explaining a lot of the terms.

```{R}

# Using the test set here, which was not used to fit the model, meaning
# this is out-of-sample validation/prediction
predictions <- testing(init_split) %>%
  mutate(pred_pr = predict(fit1, x_test))

# AUC (area under the ROC curve) is a common way to assess a binary prediction
# 0.5 = model no better than a random classifier; the closer AUC gets to 
# 1 (a perfect classifier), the better the model
predictions %>%
  mutate(truth = Class) %>%
  roc_auc(pred_pr, truth = truth, estimator = "binary", event_level = "first")

# Confusion matrix for predictions
# A confusion matrix is a 2x2 matrix:
# True Positives | False Positive
# ---------------|----------------
# False Negative | True Negative

predictions %>%
  mutate(truth = Class,
         pred = factor(if_else(pred_pr >= 0.5, 
                               "recurrence-events",
                               "no-recurrence-events"), 
                       levels = c("recurrence-events", 
                                  "no-recurrence-events"))) %>%
  yardstick::conf_mat(estimate = pred, truth = truth)

# The yardstick package has many functions to evaluate (and plot evaluation) 
# of models, and works in the tidymodels package
# Try looking into some of them, and compare which ones make sense for
# a classification model.
# Hint: yardstick:: will auto-complete to what functions are available in the package

```


# Hyperparameter Tuning and Tidymodels

In general, ML algorithms have various hyperparameters that have a substantial impact on the quality of the resulting model fit. One strategy to find good hyperparameters is to try various combinations using cross-validation and then select the best one before refitting the model. Using separate training and testing is very important to prevent overfitting. We will also use the tidymodels package extensively now to help organize all the model we will fit.

```{R}

# Slightly tweak the formatting of the dataset for tidymodels format
# Also, for simplification of code, we'll just impute missing values
# Even though in the previous example xgboost cleanly handles them 
recipe_xgboost_tidymodels <- recipes::recipe(
  Class ~ age_f + menopause + tumor_size_f + inv_nodes_f + node_caps + 
    deg_malig + breast + breast_quad + irradiat, 
                data = training(init_split)) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_ordinalscore(age_f, tumor_size_f, inv_nodes_f) %>%
  step_dummy(breast, breast_quad, irradiat, menopause, node_caps, one_hot = T) 
  
# Using the recipes again
estimated_preprocessing2 <- prep(recipe_xgboost_tidymodels, training(init_split))

bake(estimated_preprocessing2, training(init_split))

# Everything that could be tuned, within reason
# But this isn't the most practical approach (curse of dimensionality)
# Also, note the tuning parameter names are slightly different in tidymodels
# compared to xgboost
# See the help function for explanation of the different arguments
xgb_spec_all <- boost_tree(
  trees = 25,
  tree_depth = tune(),
  min_n = tune(),
  loss_reduction = tune(),        
  sample_size = tune(), 
  mtry = tune(),        
  learn_rate = tune()      
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")


# Here's a more limited and realistic set of parameters to tune
# We fix most of the parameters, and tune (with tune())
# tree_depth and min_n
# tune() is essentially a placeholder for the value,
# and will be fixed later

# Try changing the fixed values or tuning other values
# See the help function for suggested ranges
# Also tune over the number of trees instead of tree depth.
# What values seem to work well?
# Alternatively, what values lead to bad results?
xgb_spec <- boost_tree(
  trees = 15,
  tree_depth = tune(),
  min_n = tune(),
  loss_reduction = 0,        
  sample_size = 1.0, 
  learn_rate = 0.3      
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# One way to select the grid of points to tune
# See the help/google for more details
# We specifically set the ranges to more
# plausible values for the dataset size
xgb_grid <- grid_latin_hypercube(
  tree_depth(range = c(2, 6)),
  min_n(range = c(1, 15)),
  size = 25
)

# Combine preprocessing and fitting into a "workflow"
# This helps manage different preprocessing steps and models
xgb_wf <- workflow() %>%
  add_recipe(recipe_xgboost_tidymodels) %>%
  add_model(xgb_spec)

# Set up cross-validation  
# 5 folds are used to reflect amount of data
cv_folds <- vfold_cv(training(init_split), v = 5,  strata = Class)

set.seed(1234)

# Question: How many models do we have to fit?

# Run the cross-validation
tuning_results <- tune_grid(
  xgb_wf,
  resamples = cv_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE, verbose = TRUE,  event_level = "first")
)

# Summary of the results
collect_metrics(tuning_results)

# Analysis of the results
# ROC AUC
best_auc <- select_best(tuning_results, "roc_auc")

# Create the final model, with actual values for the hyperparameters 
final_wf <- finalize_workflow(
  xgb_wf,
  best_auc
)

# Re-fit the model using the best set of hyper-parameters found by the cross-validation
final_fit <- final_wf %>%
  last_fit(init_split) 

# Some common ways to examine the final fit
final_fit %>%
  collect_predictions() %>% 
  roc_curve(Class, `.pred_recurrence-events`, event_level = "first") %>% 
  autoplot()

# The ROC (receiver operating characteristic) assess the performance across the range of classification thresholds 
# Question: what are reasons to not use 50% as the cutoff?
final_fit %>%
    collect_predictions() %>% 
    roc_auc(Class, `.pred_recurrence-events`, event_level = "first")

final_fit %>%
    collect_predictions() %>% 
    accuracy(Class, .pred_class)

# Question: Is this accuracy good for this dataset? What accuracy would a very naive algorithm have? 

# Answer: Well a model always predicting no events would have approx 70% accuracy
# naive model of no events. Not modeling per se, but I think is important
# for building intuition around the topic.

# To help answer this, build a confusion matrix for this model and the naive model (i.e. always predict no recurrence).

# Remove This
# Confusion matrix for the model
final_fit %>%
    collect_predictions() %>% 
    conf_mat(Class, .pred_class)

# Null model confusion matrix
testing(init_split) %>%
  mutate(null_model_prediction = first(Class[Class == "no-recurrence-events"])) %>%
  conf_mat(truth = "Class", estimate = null_model_prediction)


# Question: What about focusing on the patients with recurrence-events?
# What are alternatives to ROC in the yardstick package for calculating an AUC?

# Question: Why are the performance metrics worse in the final fit than in the 
# cross-validation

# The precision-recall curve focuses on the prediction of one event type of interest. 
# It is related to the ROC curve
# For a given threshold, the precision is the rate of correct predictions of the class,
# and the recall is the rate of all the class instances predicted.
final_fit %>%
  collect_predictions() %>% 
  # set event_level = "first" because our event of interest is the 
  # `no-recurrence-event` factor level of Class, which is the first
  # level in Class.
  pr_curve(Class, `.pred_recurrence-events`, event_level = "first") %>% 
  autoplot()

final_fit %>%
  collect_predictions() %>% 
  # set event_level = "first" because our event of interest is the 
  # `no-recurrence-event` factor level of Class, which is the first
  # level in Class.
  pr_auc(Class, `.pred_recurrence-events`, event_level = "first")

# We can look at the sequence of trees fit by xgboost for the final model
# Here's the first one
# This is output in the Rstudio viewer
xgboost::xgb.plot.tree(model = extract_fit_parsnip(final_fit)$fit, trees = 0)

# EXERCISE: Output the first 3 trees. How do you interpret this sequence of trees
```

Model evaluation for binary endpoints is often challenging regardless of the modeling strategy. Here, we'll use a calibration table to assess the predicted probabilities versus observed probabilities across the range of the data. Think of this like a predicted vs. observed plot for continuous data. Calibration is the idea that if we predict a 70% probability for something, the event should in fact happen 70% of the time. In fact, tree-based models often have difficulty predicting on the extreme ranges of the data (why?),  but we don't really have enough data here to see that very well. Ideally, we'd create a lot of different buckets to assess calibration probability in, but there's only a moderate amount of data in the test set, so 4 are used here to illustrate the concept. 

```{R}

final_fit %>%
  collect_predictions() %>% # Like the base R predict function
  mutate(predicted_quartile = ntile(`.pred_recurrence-events`, 4)) %>% # Create the 4 buckets
  group_by(predicted_quartile) %>%
  summarise(n = n(), 
            observed = mean(Class == "recurrence-events"),
            predicted = mean(`.pred_recurrence-events`))
```

# Bonus Section: Imbalanced Classes

Modeling situations where one class of data we're trying to make predictions for is happens rarely in the dataset. By default, every instance of the data is weighted the same, which means the algorithm will put more weight on correctly predicting the common class, and predictions of the rare class can be poor. One approach is to reweight the training data such that each class is equally weighted. 

```{R}

set.seed(1234)

# This recipe includes the pre-computed weights (case_weight) in the formula 
recipe_xgboost_tidymodels_weights <- recipes::recipe(
  Class ~ age_f + menopause + tumor_size_f + inv_nodes_f + node_caps + 
    deg_malig + breast + breast_quad + irradiat +
    case_weight, 
                data = training(init_split)) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_ordinalscore(age_f, tumor_size_f, inv_nodes_f) %>%
  step_dummy(breast, breast_quad, irradiat, menopause, node_caps, one_hot = T) 

# Using the update function for slightly different models
# reduces the copy-pasting of code
xgb_wf_weights <- xgb_wf %>%
  update_recipe(recipe_xgboost_tidymodels_weights) %>%
  add_case_weights(case_weight)

# TODO: As before, use cross validation to tune the model hyperparameters
# Use precision-recall tuning_results_weights <- tune_grid(
  xgb_wf_weights,
  resamples = cv_folds,
  grid = xgb_grid,
  metrics = metric_set(pr_auc), # Use precision-recall 
  control = control_grid(save_pred = TRUE, verbose = TRUE,  event_level = "first")
)

# We're using the precision-recall AUC
# because we care more about identifying the 
# patients with recurrence events
collect_metrics(tuning_results_weights)

# Use the precision-recall AUC to pick the best model
best_auc_weighted <- select_best(tuning_results_weights, metric = "pr_auc")


# Create the final model, with actual values for the hyperparameters 
final_wf_weighted <- finalize_workflow(
  xgb_wf_weights,
  best_auc_weighted
)

# As before, refit using the best hyperparameters
final_fit_weighted <- final_wf_weighted %>%
  last_fit(init_split) 

# TODO: Make a precision-recall curve
# Remove this for exercise
final_fit_weighted %>%
  collect_predictions() %>% 
  pr_curve(Class, `.pred_recurrence-events`, event_level = "first") %>% 
  autoplot()

# TODO: Calculate the precision-recall AUC
# Remove this for exercise
final_fit_weighted %>%
    collect_predictions() %>% 
    pr_auc(Class, `.pred_recurrence-events`, event_level = "first")

# Calculate the ROC AUC.
# Remove this for exercise
# How does this compare to the previous model
final_fit_weighted %>%
    collect_predictions() %>% 
    roc_auc(Class, `.pred_recurrence-events`, event_level = "first")

# Question: How has using the class weights changed each of the performance metrics
# (ROC AUC, PR AUC, Accuracy)
# Consider how the goals of the model inform these choices
```


# Bonus Section: Model interpretation with Shapley Values

Shapley values are one strategy for interpreting ML models. For tree-based models (e.g., xgboost, random forests) they are especially computationally efficent. The intuition is to recover a set of additive "forces" that explain the difference between the prediction of a given data point and the mean prediction, by assigning an effect to each feature (covariate), which add up to that difference. 

Here's two moderately technical references for shapley valyes and SHAP: https://christophm.github.io/interpretable-ml-book/shap.html and https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html



```{R}

# Note the shapley values are for predicting the first class in the Class factor, which is no recurrence events
# Always good to pay attention to how R and libraries are using factor orderings
# The test data is used, in case the model was overfit
shapley_input_data <- bake(extract_recipe(final_fit), testing(init_split)) %>% select(-Class) %>% as.matrix()

shap_plot <- xgb.ggplot.shap.summary(data = shapley_input_data,
                        model = extract_fit_parsnip(final_fit)$fit) +
  labs(x = "Shapley Value", color = "Feature Value") 


# The plot uses the color to code the feature quantile,
# the x-axis is the shapley value. 
# Each point is one observation in the test set, 
# and so each row on the graph has the same number of points
# as data rows in the test set.

shap_plot

# There are additional ways to visualize shapley values and interactions with covariates,
# but is beyond the scope of this webinar.
# N.B. many of the libraries were developed first for Python, and then ported to R
# e.g. fastshap

# Question: What does the shapley plot look like using overfit model on the training vs testing sets?

# Excercise: Another common shapley plot has a covariate on the x-axis, 
# the shapley value on the y axis, and a second covariate as the color
# to explore interaction effects. Using this style of plot, 
# what interactions are interesting? 
```

We also sanity check the shapley predictions by making predictions for two different values of tumor sizes for one set of covariate values. This is analogous to plotting the predictions for a reference individual and permuting the one covariate.

```{R}

# Create a dataset of covariates to make predictions
x_predict <- testing(init_split)[1,] %>%
  select(-tumor_size_f) %>%
  dplyr::inner_join(tibble(tumor_size_f = c("0-4", "50-54")), by = character())


# Make the predictions
predict(extract_workflow(final_fit),
        new_data = x_predict,
        type = "prob")
```


## References

https://en.wikipedia.org/wiki/Gradient_boosting
https://www.tidymodels.org/
https://xgboost.readthedocs.io/en/stable/index.html
https://juliasilge.com/blog/xgboost-tune-volleyball/